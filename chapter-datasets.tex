%======================================================================
\chapter{Implementations}
%======================================================================

\section{Datasets}

%\myworries{Add statistics and examples? Elaborate on splits? Add motivation, gist of each dataset Add length distribution}

\subsection{Fine-Tuning}

In order to learn a model of relevance with BERT, we need training pairs of short text and relevance judgements.
A number of collections fortuitously contain relevance judgements at the sentence and passage level, which makes them the ideal choice for fine-tuning BERT.
We fine-tune BERT on three such sentence- and passage-level datasets individually and in combination:\ TREC Microblog, MicroSoft MAchine Reading Comprehension and TREC Complex Answer Retrieval datasets.
The details of each dataset are provided below.

\subsubsection{TREC Microblog (MB)}

\begin{figure}[b!]
	\begin{framed}
		\centering
    		\textbf{Query:} bbc world service staff cuts \\
    		\textbf{Text:} irish times : bbc world service confirms cuts : the bbc world service will shed around 650 jobs or more \\
    		\textbf{Relevance:} 1 (``relevant'')
	\end{framed}
\label{mb-example}
 \caption{An example of... MB}
\end{figure}

\begin{table}[t]
\vspace{0.2cm}
\centering
\begin{tabular}{llll}
\toprule
\textbf{Type} \mbox{\hspace{0.5cm}} & \textbf{Training Set} \mbox{\hspace{1.0cm}} & \textbf{Validation Set} \mbox{\hspace{1.0cm}} & \textbf{Total} \mbox{\hspace{1.0cm}} \\
\toprule
Number of queries & 166 & 59 & 225 \\
Number  of tweets & asd & asd & asd \\
Percentage of relevant tweets  & asd      & asd & asd   \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{...}% at the $p<0.01$ level.}
\label{tab:mb-stats}
\vspace{-0.6cm}
\end{table}

The TREC Microblog dataset draws from the Microblog Tracks at TREC from 2011 to 2014, with topics and relevance judgments over tweets.
Topics associated with tweets are treated as queries, and each dataset contains 50 queries.
The nature of this collection differs from newswire documents that we evaluate our models on in distinct ways:\
First of all, tweets in MB have a much shorter length than those of newswire documents.
By definition, tweets are limited to 280 characters.
The length distribution of tweets in MB is displayed in \myworries{Figure X}.
Furthermore, because queries and tweets are comparable in length, exact matches of query terms occur less frequently in the tweets than they might in longer documents such as news articles.
Therefore, semantic matching signals may take precedence in improving retrieval effectiveness.
Related to this point, tweets are expressed in a much less formal language than news articles.
Tweets may characteristically contain various abbreviations (partly due to the length constraint), informal conventions such as hashtags or typos.
Such informal language may result in term mismatches in the case of exact matching.
It may therefore be helpful to catch other semantic signals with a deep neural network.

We use the MB data prepared by Rao et al. \cite{rao2019tweet}.
We extract the queries, tweets and relevance judgements from the dataset, excluding metadata such as query time and URLs of the tweets.
Relevance judgements in MB are in fact reported on a three-point scale where (``irrelevant'', ``relevant'' and ``highly relevant''); however, for the purposes of this work we treat both higher degrees of relevance as equal \cite{ounisoverview}.
Both queries and tweets are segmented into token sequences with the Stanford Tokenizer Tool\footnote{https://nlp.stanford.edu/software/tokenizer.shtml}.
We \myworries{sample} 25\% of the data for the validation set, and use the rest for fine-tuning BERT.
We experiment with different splits as discussed in Section \myworries{Exp-results}, and find this split to be ideal.

\subsubsection{MicroSoft MAchine Reading Comprehension (MS MARCO)}

\begin{table}[b]
\vspace{0.2cm}
\centering
\begin{tabular}{llll}
\toprule
\textbf{Type} \mbox{\hspace{0.5cm}} & \textbf{Training Set} \mbox{\hspace{1.0cm}} & \textbf{Validation Set} \mbox{\hspace{1.0cm}} & \textbf{Total} \mbox{\hspace{1.0cm}} \\
\toprule
Number of queries & 12.8M & asd & asd \\
Number  of ? & asd & asd & asd \\
Percentage of relevant passages  & asd      & asd & asd   \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{...}% at the $p<0.01$ level.}
\label{tab:marco-stats}
\vspace{-0.6cm}
\end{table}

\begin{figure}[b!]
	\begin{framed}
		\centering
    		\textbf{Query:} bbc world service staff cuts \\
    		\textbf{Relevant Passage:} irish times : bbc world service confirms cuts : the bbc world service will shed around 650 jobs or more \\
    		\textbf{Non-relevant Passage:} irish times : bbc world service confirms cuts : the bbc world service will shed around 650 jobs or more \\
	\end{framed}
\label{marco-example}
 \caption{\myworries{TODO} An example of... MS MARCO}
\end{figure}

MS MARCO is a large-scale machine reading comprehension and question answering dataset that is extensively used in the NLP community.
MS MARCO \cite{nguyen2016msmarco} features user queries sampled from Bingâ€™s search logs and passages extracted from web documents.
The dataset is composed of tuples of a query with relevant and non-relevant passages.
On average, each query has one relevant passage.
However, some may have no relevant passage at all as the dataset is constructed from  top-10 passages manually annotated by human judges.
Therefore, some relevant passages might not have been retrieved with BM25.
MS MARCO can be distinguished from similar datasets by its size and real-world nature.
Similar to MB, MS MARCO is representative of a natural, and noisy, distribution of information needs, unlike other datasets that often contain high-quality text that may not reflect the use in real life.
\myworries{What else? Robust systems}

Here we focus on the passage-ranking task on MS MARCO.
Following the settings in Nogueira et al.~\cite{}, we train BERT on approximately 400M training samples.
The development set is composed of approximately 6.9k queries, each paired with the top 1000 most relevant passages in the MS MARCO dataset as retrieved with BM25.
Similarly, the evaluation set contains approximately 6.8 queries and their top 1000 passages, but without the relevance annotations.
The models in Section \myworries{X} were trained on less than 2\% of the total training set (~12.8M) due to the size of the dataset and time required to train on it even on TPUs.
According to Nogueira et al. \cite{nogueira2019passage}, training for up to 12.5\% of the total data does not improve MRR@10 on the validation set.

\subsubsection{TREC Complex Answer Retrieval (CAR)}


TREC CAR \cite{dietz2017trec} uses paragraphs extracted from all English Wikipedia paragraphs, except the abstracts.
\myworries{How many queries?}
Each query is formed by concatenating an article title and a section heading, with all passages under that section considered relevant.
The organizers of TREC CAR 2017 only provide manual annotations for the top-5 passages retrieved, meaning some relevant passages may not be annotated if they rank lower.
For this reason, we opt to use automatic annotations that provide relevance judgements for all possible query-passage pairs.
The goal of this TREC track is to automatically collect and condense information for a complex query into a single coherent summary.
Rather than focusing on document retrieval, the priority is aggregating synthesized information in the form of references, facts and opinions.
However, CAR is a synthetic dataset in the sense that queries and documents do not reflect real-world distributions or information needs.
\myworries{More?}

The dataset has five predefined folds over the queries.
Paragraphs corresponding to the first four folds are used to construct the training set (consisting of approximately 3M queries), and the rest the validation set (approximately 700K queries).
The original test set used to evaluate submissions to TREC CAR 2019 is used for testing purposes (approximately 1.8k queries).
The official BERT models are pre-trained on the entire Wikipedia dump; therefore, they have also been trained on documents in the TREC CAR test collection albeit in an unsupervised fashion.
In order to avoid the leak of test data into training, we use the BERT model pre-trained only on the half of Wikipedia present in CAR training samples~\cite{}.
30M fine-tuning query-passage pairs were generated by retrieving the top 10 passages from the entire CAR corpus with BM25.
Similar to MS MARCO, training on more than 40\% of the training set did not lead to any improvements on the validation set.

\begin{table}[b]
\vspace{0.2cm}
\centering
\begin{tabular}{llll}
\toprule
\textbf{Type} \mbox{\hspace{0.5cm}} & \textbf{Training Set} \mbox{\hspace{1.0cm}} & \textbf{Validation Set} \mbox{\hspace{1.0cm}} & \textbf{Total} \mbox{\hspace{1.0cm}} \\
\toprule
Number of queries & 12.8M & asd & asd \\
Number  of ? & asd & asd & asd \\
Percentage of relevant passages  & asd      & asd & asd   \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{...}% at the $p<0.01$ level.}
\label{tab:car-stats}
\vspace{-0.6cm}
\end{table}

\myworries{TODO}
\begin{figure}[b!]
	\begin{framed}
		\centering
    		\textbf{Query:} bbc world service staff cuts \\
    		\textbf{Text:} irish times : bbc world service confirms cuts : the bbc world service will shed around 650 jobs or more \\
    		\textbf{Relevance:} 1 (``relevant'')
	\end{framed}
\label{car-example}
 \caption{An example of... CAR}
\end{figure}

\subsection{Evaluation}

We conduct end-to-end document ranking experiments on three TREC newswire collections:\
the Robust Track from 2004 (Robust04) and the Common Core Tracks from 2017 and 2018 (Core17 and Core18).

\subsubsection{Robust04}

Robust04 draws from the TREC Robust Track in 2004, which is the set of documents from TREC Disks 4 and 5, spanning news articles from Financial Times and LA Times, except the Congressional Record.
The dataset comprises 250 topics, with relevance judgments on a collection of 500K documents.
The goal of the Robust track is to improve the consistency and robustness of retrieval methods by focusing search on poorly performing topics .
Specifically, this task involves searching across a fixed set of documents using previously unseen topics.
Notably the lengths of documents in Robust04 are highly biased:\ \myworries{data}, which is difficult for neural text matching models to handle.
\myworries{What does this mean for us?}

\subsubsection{Core17 \& Core18}

Core17 and Core18 are based on the TREC 2017 and 2018 Common Core Tracks respectively.
The motivation behind these tracks is to build up-to-date test collections based on more recently created documents.
\myworries{that avoids the pitfalls of depth-k pooling}
Core17 uses 1.8M articles from the New York Times Annotated Corpus
while Core18 uses around 600K articles from the TREC Washington Post Corpus.
Core17 and Core18 have only 50 topics each, which are drawn from the Robust Track topics.

\section{Evaluation Metrics}

Evaluation in information retrieval relies on the distinction between ``relevant'' and ``irrelevant'' documents with respect to an information need as expressed by a query.
A number of automatic evaluation metrics has been formalized specifically for ranking tasks, some of which are described below.

%The size of most document collections makes it infeasible for humans to manually judge the relevance of all documents.
%All relevant documents need to be labelled to prevent false negatives, i.e: treating documents which are in fact relevant as irrelevant.

\subsection{Mean Average Precision (MAP)}

Precision specifies what fraction of a set of retrieved documents is in fact relevant for a given query $ q $.
By extension, average precision (AP) expresses the average of the precision values obtained for the set of top $ k $ documents for the query.
Support that $ D = \{d_1, ..., d_{m_j}\} $ is the set of all relevant documents for a query $ q_j $, then AP can be formulated as:

\begin{equation}
AP = \frac{1}{m_j} \sum^{m_j} _{k = 1} P(R_{jk})
\end{equation}

where $ R_{jk} $ represents the set of top $ k $ ranked retrieval results.

The respective AP for each query $ q_{j} \in Q $ can be aggregated to obtain mean average precision (MAP) for the overall retrieval effectiveness in the form of a single-figure measure of quality across various recall levels:

\begin{equation}
MAP = \frac{\sum^{|Q|} _{j = 1} AP}{Q} = \frac{1}{Q} \sum^{|Q|} _{j = 1} \frac{1}{m_j} \sum^{m_j} _{k = 1} P(R_{jk})
\end{equation}

%\myworries{Meaning of MAP: To recapitulate, if the AP is 0.5, the relevant
%answer occurs at every second position. If the AP score is 0.3, the relevant answer occurs
%at every 3rd position and an AP score of 0.1 would mean that every 10th answer is correct.}

MAP is known to have especially good discrimination and stability compared to other evaluation metrics, which makes it the ideal choice for large text collections~\cite{manning2010introduction}.
It is hence one of the standard metrics among the TREC community.

\subsection{Precision at k (P@k)}

While MAP factors in precision at all recall levels, certain applications may have a distinctly different notion for ranking quality.
Particularly in the case of web search, the user often only cares about the results on the first page or two.
This restriction essentially requires measuring precision at fixed low levels of retrieved results, i.e: top $ k $ documents -- hence the name for the metric ``precision at $ k $''.
On the one hand, it eliminates the need for any estimate of the size of the set of relevant documents because it is only concerned with the top documents.
However, it also produces the least stable results out of all evaluation metrics.
Moreover, precision at $ k $ does not average well because it is too sensitive to the total number of relevant documents for a query.

\subsection{Normalized Discounted Cumulative Gain (NDCG@k)}

Cumulative gain (CG) simply computes the sum of relevance labels for all the retrieved documents, treating the search results as an unordered set.
However, since a highly relevant document is inherently more useful when it appears higher up in the search results, CG has been extended to discounted cumulative gain (DCG).
DCG estimates the relevance of a document based on its rank among the retrieved documents.
The relevance measure is accumulated from top to bottom, discounting the value of documents at lower ranks.
NDCG at $ k $ measures DCG for the top $ k $ documents, normalizing by the highest possible value for a query; therefore, a perfect ranking yields NDCG equals 1.

NDCG is uniquely useful in applications with a non-binary notion of relevance, e.g: a spectrum of relevance.
This makes NDCG comparable across different queries:\
The NDCG values for all queries can be averaged to reliably evaluate the effectiveness of a ranking algorithm for various information needs across a collection.
Given a set of queries $ q_j \in Q $ and relevance judgements $ R_{dj} $ for a document $ d $:\

\begin{equation}
NDCG(Q, k) = \frac{1}{|Q|} \sum ^{|Q|} _{j = 1} Z_{kj} \sum ^k _{m = 1} \frac{2^{R_{jm}} - 1}{log_2 (1 + m)'}
\end{equation}

where $ Z_{kj} $ is the normalization factor.