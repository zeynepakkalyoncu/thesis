\chapter{Datasets}

\section{Fine-Tuning}

In order to model sentence-level relevance with BERT, we need training pairs of queries and short text, annotated with relevance labels.
Fortunately, a number of collections fortuitously contain such relevance judgements at the sentence and passage level, which makes them the ideal choice for training our model.
We fine-tune BERT on three such sentence- and passage-level datasets individually and in combination:\ TREC Microblog~\cite{ounisoverview}, MicroSoft MAchine Reading Comprehension~\cite{nguyen2016msmarco} and TREC Complex Answer Retrieval~\cite{dietz2017trec} datasets.
The details of each dataset are provided below.

\subsection{TREC Microblog (MB)}

\begin{figure}[b!]
	\begin{framed}
		\centering
    		\textbf{Query:} bbc world service staff cuts \\
    		\textbf{Text:} irish times : bbc world service confirms cuts : the bbc world service will shed around 650 jobs or more \\
    		\textbf{Relevance:} 1 (``relevant'')
	\end{framed}
\label{mb-example}
 \caption{A sample relevant query and tweet pair from the MB dataset.}
\end{figure}

\begin{table}[t]
\vspace{0.2cm}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Type} \mbox{\hspace{0.5cm}} & \textbf{Training Set} \mbox{\hspace{1.0cm}} & \textbf{Validation Set} \mbox{\hspace{1.0cm}} \\
\toprule
Number of queries & 166 & 59 \\
Number  of tweets & 133K & 44K  \\
%Percentage of relevant tweets  & 7\% & 16\%  \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{Statistics about the MB dataset.}
\label{tab:mb-stats}
%\vspace{-0.6cm}
\end{table}

The TREC Microblog dataset draws from the Microblog Tracks at TREC from 2011 to 2014, with topics and relevance judgments over tweets.
Topics associated with tweets are treated as queries, and each of the four datasets contains approximately 50 queries.
The nature of this collection differs from newswire documents that we evaluate our models on in distinct ways:\
First of all, tweets have much fewer tokens than newswire documents.
By definition, tweets are limited to 280 characters.
%The length distribution of tweets in MB is displayed in \Figure X}.
Furthermore, because queries and tweets in this dataset are comparable in length, exact matches of query terms occur less frequently in the tweets than they might in longer documents such as news articles.
Therefore, semantic matching signals may take precedence in improving retrieval effectiveness on MB.
%\myworries{How is this relevant to our training? It's valuable because...}
Related to this point, tweets are expressed in a much less formal language than news articles.
Tweets may characteristically contain various abbreviations (partly due to the aforementioned length constraint), informal conventions such as hashtags or typos.
Such informal language may result in term mismatches in the case of exact matching.
It may therefore be helpful to catch other semantic signals with a deep neural network.

We use the MB data prepared by Rao et al.~\cite{rao2019tweet}.~\footnote{https://github.com/jinfengr/neural-tweet-search}
We extract the queries, tweets and relevance judgements from the dataset, excluding metadata such as query time and URLs of the tweets.
Relevance judgements in MB are reported on a three-point scale where (``irrelevant'', ``relevant'' and ``highly relevant''); however, for the purposes of this work we treat both higher degrees of relevance as equal~\cite{ounisoverview}.
Both queries and tweets are segmented into token sequences.
We sample 25\% of the data for the validation set, and use the rest for fine-tuning BERT.
We experiment with different splits as discussed in Chapter \ref{ch:results}, and find this split to be ideal.

\subsection{MicroSoft MAchine Reading Comprehension (MS MARCO)}

\begin{table}[b]
\vspace{0.2cm}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Type} \mbox{\hspace{0.5cm}} & \textbf{Training Set} \mbox{\hspace{1.0cm}} & \textbf{Validation Set} \mbox{\hspace{1.0cm}} \\
\toprule
Number of queries & 809K & 6.9K \\
Number  of passages & 12.M & 6.9M \\
%Percentage of relevant passages  & asd & asd \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{Statistics about the MS MARCO dataset.}
\label{tab:marco-stats}
\end{table}

\begin{figure}[b!]
	\begin{framed}
%		\centering
    		\textbf{Query:} is a little caffeine ok during pregnancy \\
    		\textbf{Relevant Passage:} We don't know a lot about the effects of caffeine during pregnancy on you and your baby. So it's best to limit the amount you get each day. If you're pregnant, limit caffeine to 200 milligrams each day. This is about the amount in 1.5 8-ounce cups of coffee or one 12-ounce cup of coffee. \\
    		\textbf{Non-relevant Passage:} It is generally safe for pregnant women to eat chocolate because studies have shown to prove certain benefits of eating chocolate during pregnancy. However, pregnant women should ensure their caffeine intake is below 200 mg per day. \\
	\end{framed}
\label{marco-example}
 \caption{A sample relevant and non-relevant passage pair for a query from the MB dataset}
\end{figure}

MS MARCO is a large-scale machine reading comprehension and question answering dataset that is extensively used in the NLP community.
MS MARCO~\cite{nguyen2016msmarco} features user queries sampled from Bingâ€™s search logs and passages extracted from web documents.
The dataset is composed of tuples of a query with relevant and non-relevant passages.
On average, each query has one relevant passage.
However, some may have no relevant passage at all as the dataset is constructed from the top-10 passages manually annotated by human judges.
Therefore, some relevant passages might not have been retrieved with BM25.
%\myworries{Rephrase this}
MS MARCO can be distinguished from similar datasets by its size and real-world nature.
Similar to MB, MS MARCO is representative of a natural, and noisy, distribution of information needs, unlike other datasets that often contain high-quality text that may not reflect the use in real life.
%\myworries{What else? Robust systems}

Here we focus on the passage-ranking dataset of MS MARCO.
Following the settings in Nogueira et al.~\cite{nogueira2019passage}, we train BERT on approximately 12.8M training samples.
The development set is composed of approximately 6.9k queries, each paired with the top 1000 most relevant passages in the MS MARCO dataset as retrieved with BM25.
Similarly, the evaluation set contains approximately 6.8 queries and their top 1000 passages, but without the relevance annotations.
%The models in Section \myworries{X} were trained on less than 2\% of the total training set (~12.8M) due to the size of the dataset and time required to train on it even on TPUs.
%According to Nogueira et al. \cite{nogueira2019passage}, training for up to 12.5\% of the total data does not improve MRR@10 on the validation set.
%\myworries{Maybe remove MRR, and better transition}

\subsection{TREC Complex Answer Retrieval (CAR)}

\begin{table}[t!]
\vspace{0.2cm}
\centering
\begin{tabular}{llll}
\toprule
\textbf{Type} \mbox{\hspace{0.5cm}} & \textbf{Training Set} \mbox{\hspace{1.0cm}} & \textbf{Validation Set} \mbox{\hspace{1.0cm}} \\
\toprule
Number of queries & 3M & 700K \\
Number of passages & 30M & 7M \\
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{Statistics about the CAR dataset.}
\label{tab:car-stats}
\end{table}

%\begin{figure}[b!]
%	\begin{framed}
%		\centering
%    		\textbf{Query:} bbc world service staff cuts \\
%    		\textbf{Text:} irish times : bbc world service confirms cuts : the bbc world service will shed around 650 jobs or more \\
%    		\textbf{Relevance:} 1 (``relevant'')
%	\end{framed}
%\label{car-example}
% \caption{\myworries{TODO}}
%\end{figure}

TREC CAR \cite{dietz2017trec} uses paragraphs extracted from all paragraphs in the English Wikipedia, except the abstracts.
Each query is formed by concatenating an article title and a section heading, with all passages under that section considered relevant.
The goal of this TREC track is to automatically collect and condense information for a complex query into a single coherent summary.
Rather than focusing on document retrieval, the priority is aggregating synthesized information in the form of references, facts, and opinions.
However, CAR is a synthetic dataset in the sense that queries and documents do not reflect real-world distributions or information needs.
The organizers of TREC CAR 2017 only provide manual annotations for the top-5 passages retrieved, meaning some relevant passages may not be annotated if they rank lower.
For this reason, we opt to use automatic annotations that provide relevance judgements for all possible query-passage pairs.
%\myworries{More?}

The dataset has five predefined folds over the queries.
Paragraphs corresponding to the first four folds are used to construct the training set consisting of approximately 3M queries, and the rest the validation set of around 700K queries.
The original test set used to evaluate submissions to TREC CAR is used for testing purposes.
A subtle detail to note is that the official BERT models are pre-trained on the entire Wikipedia dump; therefore, they have also been trained on documents in the TREC CAR test collection albeit in an unsupervised fashion.
In order to avoid the leak of test data into training, we use the BERT model pre-trained only on the half of Wikipedia present in CAR training samples~\cite{nogueira2019passage}.
The training pairs for CAR are generated by retrieving the top 10 passages from the entire CAR corpus with BM25.
%30M fine-tuning query-passage pairs were generated by retrieving the top 10 passages from the entire CAR corpus with BM25.
%Similar to MS MARCO, training on more than 40\% of the training set did not lead to any improvements on the validation set

\section{Evaluation}

We conduct end-to-end document ranking experiments on three TREC newswire collections:\ the Robust Track from 2004 (Robust04)~\cite{Voorhees_TREC2004_robust} and the Common Core Tracks from 2017 and 2018 (Core17~\cite{allan2017trec} and Core18~\cite{core2018trec}).

\subsubsection{Robust04}

Robust04 draws from the set of documents in TREC Disks 4 and 5, spanning news articles from Financial Times and LA Times, except the Congressional Record.
The collection comprises 250 topics, with relevance judgments over 500K documents.
The goal of the Robust track is to improve the consistency and robustness of retrieval methods by focusing ``ad hoc'' search on poorly performing topics~\cite{Voorhees_TREC2004_robust}.
%The task involves searching across a fixed set of documents using previously unseen topics.
Notably the distribution of document lengths in Robust04 is highly skewed, with the majority of documents having fewer than 200K tokens and a couple of documents having more than 1M tokens.
Existing neural re-rankers such as DRMM~\cite{guo2016deep} and KNRM~\cite{xiong2017knrm} are known to struggle with documents in the tail of the distribution.

\subsubsection{Core17 \& Core18}

Core17 and Core18 build on the TREC 2017 and 2018 Common Core Tracks respectively.
The motivation behind these tracks is to build up-to-date test collections based on more recently created documents.
%\myworries{that avoids the pitfalls of depth-k pooling}
Core17 contains 1.8M articles from the New York Times Annotated Corpus while Core18 has around 600K articles from the TREC Washington Post Corpus.
Core17 and Core18 have only 50 topics each, which are drawn from the Robust Track topics.
Given their relatively recent release, literature on these collections is still sparse.