\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces An example of a query-text pair from the TREC Robust04 collection where a relevant piece of text does not contain direct query matches.}}{1}{figure.1.1}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces The architecture combining BERT an additional output layer for the sentence pair classification task, where $ E $ represents the input embedding for each sentence and $ T_i $ the contextual representation of token $ i $. Adapted from Delvin et al.\nobreakspace {}\cite {devlin2018bert}.}}{11}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces The two types of deep matching architectures: representation-focused (a) and interaction-focused (b). Adapted from Guo et al.\nobreakspace {}\cite {guo2017drmm}.}}{13}{figure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Visualization of best AP scores on Robust04 for 108 papers based on non-neural and neural approaches. Adapted from Yang et al.\nobreakspace {}\cite {Yang_etal_SIGIR2019}.}}{17}{figure.2.3}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces \leavevmode {\color {red}TODO}}}{20}{figure.3.1}
\contentsline {figure}{\numberline {3.2}{\ignorespaces \leavevmode {\color {red}TODO}}}{21}{figure.3.2}
\contentsline {figure}{\numberline {3.3}{\ignorespaces \leavevmode {\color {red}TODO}}}{23}{figure.3.3}
\contentsline {figure}{\numberline {3.4}{\ignorespaces \leavevmode {\color {red}Put tokenized BERT input example?}}}{24}{figure.3.4}
\contentsline {figure}{\numberline {3.5}{\ignorespaces \leavevmode {\color {red}Create customized diagram}}}{24}{figure.3.5}
\contentsline {figure}{\numberline {3.6}{\ignorespaces ...}}{26}{figure.3.6}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces \leavevmode {\color {red}Update}}}{29}{figure.4.1}
\addvspace {10\p@ }
\addvspace {10\p@ }
