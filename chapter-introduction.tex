%======================================================================
\chapter{Introduction}
%======================================================================
\label{intro}

Document retrieval refers to the task of generating a ranking of documents from a large corpus $ D $ in response to a query $ Q $.
In a typical document retrieval pipeline, an inverted index is constructed in advance from the collection, which often comprises unstructured text documents, for fast access during retrieval.
When the user issues a query, the query representation is matched against the index, computing a similarity score for each document.
The top most relevant documents based on their closeness to the query are returned to the user in order of relevance.
This procedure may be followed by a subsequent re-ranking stage where the candidate documents outputted by the previous step are further re-ranked in a way that maximizes some retrieval metric such as average precision (AP).

\begin{figure}[b!]
	\begin{framed}
		\centering
    		\textbf{Query:} international art crime \\
    		\textbf{Text:} The thieves demand a ransom of \$2.2 million for the works and return one of them.
	\end{framed}
\label{query-sent-example}
 \caption{An example of a query-text pair from the TREC Robust04 collection where a relevant piece of text does not contain direct query matches.}
\end{figure}

Document retrieval systems traditionally rely on term-matching techniques, such as BM25, to judge the relevance of documents in a corpus.
More specifically, the more common terms a document shares with the query, the more relevant it is considered.
As a result, these systems may fail to detect documents that do not contain exact query terms, but are nonetheless relevant.
For example, consider a document that expresses relevant information in a way that cannot be resolved without external semantic analysis.
Figure \ref{query-sent-example} displays one such query-text pair where words semantically close to the query need to be identified to establish relevance.
This ``vocabulary mismatch'' problem represents a long-standing challenge in information retrieval.
To put its significance into context, Zhao et al. \cite{zhao2010term} show in their paper on term necessity prediction that, statistically, the average query terms do not appear in as many as 30\% of relevant documents in TREC 3 to 8 ``ad hoc'' retrieval datasets.

Clearly, the classic exact matching approach to document retrieval neglects to exploit rich semantic information embedded in the document texts.
To overcome this shortcoming, a number of models such as Latent Semantic Analysis \cite{deerwester1990indexing}, which map both queries and documents into high-dimensional vectors, and measure closeness between the two based on vector similarity, has been proposed.
This innovation has enabled semantic matching to improve document retrieval by extracting useful semantic signals.
With the advent of neural networks, it has become possible to learn better distributed representations of words that capture more fine-grained semantic and syntatic information \cite{mikolov2013distributed}, \cite{pennington2014glove}.
More recently, massively unsupervised language models that learn context-specific semantic information from copious amounts of data have changed the tide in NLP research (e.g: ELMo \cite{peters2018deep}, GPT-2 \cite{radford2019language}).
These models can be applied to various downstream tasks with minimal task-specific fine-tuning, highlighting the power of transfer learning from large pre-trained models.
Arguably the most popular example of these deep language representation models is the Bidirectional Encoder Representations from Transformers (BERT) \cite{devlin2018bert}.
BERT has achieved state-of-the-art results across a broad range of NLP tasks from question answering to machine translation.

While BERT has enjoyed widespread adoption across the NLP community, its application in information retrieval research has been limited in comparison.
Guo et al. \cite{guo2016deep} suggest that the lackluster success of deep neural networks in information retrieval may be owing to the fact that they often do not properly address crucial characteristics of the ``ad hoc'' document retrieval task.
Specifically, the relevance matching problem in information retrieval and semantic matching problem in natural language processing are fundamentally different in that the former depends heavily on exact matching signals, query term importance and diverse matching requirements.
In other words, it is crucial to strike a good balance between exact and semantic matching in document retrieval.
For this reason, we employ both document scores based on term-matching and semantic relevance scores to determine the relevance of documents.

In this thesis, we extend the work of Yang et al. \cite{yang2019simple} by presenting a novel way to apply BERT to ``ad hoc'' document retrieval on long documents -- particularly, newswire articles -- with significant improvements.
Following Nogueira et al. \cite{nogueira2019passage}, we adapt BERT for binary relevance classification over text to capture notions of relevance.
We then deploy the BERT-based re-ranker as part of a multi-stage architecture where an initial list of candidate documents is retrieved with a standard bag-of-words term matching technique.
The BERT model is used to compute a relevance score for each constituent sentence, and the candidate documents are re-ranked by combining sentence scores with the original document score.

We emphasize that applying BERT to document retrieval on newswire documents is not trivial due to two main challenges:\
Firist of all, BERT has a maximum input length of 512 tokens, which is insufficient to accommodate the overall length of most news articles.
To put this into perspective, a typical TREC Robust04 document has a median length of 679 tokens, and in fact, 66\% of all documents are longer than 512 tokens.
Secondly, most collections provide relevance judgements only at the document level.
Therefore, we only know what documents are relevant for a given query, but not the specific spans within the document.
To further aggravate this issue, a document is considered relevant as long as some part of it is relevant, and most of the document often has nothing to do with the query.

We address the abovementioned challenges by proposing two effective innovations:\
First, instead of relying solely on document-level relevance judgements, we aggregate sentence-level evidence to rank documents.
As mentioned before, since standard newswire collections lack sentence level judgements to facilitate this approach, we instead explore leveraging sentence-level or passage-level judgements already available in collections in other domains, such as tweets and reading comprehension.
To this end, we fine-tune BERT models on these out-of-domain collections to learn models of relevance.
Surprisingly, we demonstrate that models of relevance can indeed be successfully transferred across domains.
%We are able to use BERT models trained on out-of-domain collections on newswire documents to compute a relevance score for each constituent sentence.
It is important to note that the representational power of neural networks come at the cost of challenges in interpretability.
For this reason, we dedicate a portion of this thesis to error analysis experiments in an attempt to qualify and better understand the cross-domain transfer effects.
We also elaborate on our engineering efforts to ensure reproducibility and replicability, and the technical challenges involved in bridging the worlds of natural language processing and information retrieval from a software engineering perspective.

\section{Evaluation Metrics}

Evaluation in information retrieval relies on the distinction between ``relevant'' and ``irrelevant'' documents with respect to an information need as expressed by a query.
A number of automatic evaluation metrics has been formalized specifically for ranking tasks, some of which are described below.

%The size of most document collections makes it infeasible for humans to manually judge the relevance of all documents.
%All relevant documents need to be labelled to prevent false negatives, i.e: treating documents which are in fact relevant as irrelevant.

\subsection{Mean Average Precision (MAP)}

Precision specifies what fraction of a set of retrieved documents is in fact relevant for a given query $ q $.
By extension, average precision (AP) expresses the average of the precision values obtained for the set of top $ k $ documents for the query.
Support that $ D = \{d_1, ..., d_{m_j}\} $ is the set of all relevant documents for a query $ q_j $, then AP can be formulated as:

\begin{equation}
AP = \frac{1}{m_j} \sum^{m_j} _{k = 1} P(R_{jk})
\end{equation}

where $ R_{jk} $ represents the set of top $ k $ ranked retrieval results.

The respective AP for each query $ q_{j} \in Q $ can be aggregated to obtain mean average precision (MAP) for the overall retrieval effectiveness in the form of a single-figure measure of quality across various recall levels:

\begin{equation}
MAP = \frac{\sum^{|Q|} _{j = 1} AP}{Q} = \frac{1}{Q} \sum^{|Q|} _{j = 1} \frac{1}{m_j} \sum^{m_j} _{k = 1} P(R_{jk})
\end{equation}

%\myworries{Meaning of MAP: To recapitulate, if the AP is 0.5, the relevant
%answer occurs at every second position. If the AP score is 0.3, the relevant answer occurs
%at every 3rd position and an AP score of 0.1 would mean that every 10th answer is correct.}

MAP is known to have especially good discrimination and stability compared to other evaluation metrics, which makes it the ideal choice for large text collections~\cite{manning2010introduction}.
It is hence one of the standard metrics among the TREC community.

\subsection{Precision at k (P@k)}

While MAP factors in precision at all recall levels, certain applications may have a distinctly different notion for ranking quality.
Particularly in the case of web search, the user often only cares about the results on the first page or two.
This restriction essentially requires measuring precision at fixed low levels of retrieved results, i.e: top $ k $ documents -- hence the name for the metric ``precision at $ k $''.
On the one hand, it eliminates the need for any estimate of the size of the set of relevant documents because it is only concerned with the top documents.
However, it also produces the least stable results out of all evaluation metrics.
Moreover, precision at $ k $ does not average well because it is too sensitive to the total number of relevant documents for a query.

\subsection{Normalized Discounted Cumulative Gain (NDCG@k)}

Cumulative gain (CG) simply computes the sum of relevance labels for all the retrieved documents, treating the search results as an unordered set.
However, since a highly relevant document is inherently more useful when it appears higher up in the search results, CG has been extended to discounted cumulative gain (DCG).
DCG estimates the relevance of a document based on its rank among the retrieved documents.
The relevance measure is accumulated from top to bottom, discounting the value of documents at lower ranks.
NDCG at $ k $ measures DCG for the top $ k $ documents, normalizing by the highest possible value for a query; therefore, a perfect ranking yields NDCG equals 1.

NDCG is uniquely useful in applications with a non-binary notion of relevance, e.g: a spectrum of relevance.
This makes NDCG comparable across different queries:\
The NDCG values for all queries can be averaged to reliably evaluate the effectiveness of a ranking algorithm for various information needs across a collection.
Given a set of queries $ q_j \in Q $ and relevance judgements $ R_{dj} $ for a document $ d $:\

\begin{equation}
NDCG(Q, k) = \frac{1}{|Q|} \sum ^{|Q|} _{j = 1} Z_{kj} \sum ^k _{m = 1} \frac{2^{R_{jm}} - 1}{log_2 (1 + m)'}
\end{equation}

where $ Z_{kj} $ is the normalization factor.

\vfill

\section{Contributions}

The main contributions of this thesis can be summarized as follows:\\

\begin{itemize}
\item
We present two innovations to successfully apply BERT to \textit{ad hoc} document retrieval with large improvements:\
integrating sentence-level evidence to address the fact that BERT cannot process long spans posed by newswire documents, and exploiting cross-domain models of relevance for collections without sentence- or passage-level annotations.
With the proposed model, we establish state-of-the-art effectiveness on three standard TREC newswire collections at the time of writing.
Our results on Robust04 exceed the previous highest known score of 0.3686 \cite{Cormack:2009:RRF:1571941.1572114} with a non-neural method based on ensembles, which has stood unchallenged for ten years.
\item
We explore through various analyses the effects of cross-domain relevance transfer with BERT as well as the contributions of BM25 and sentence scores to the final document ranking.
We investigate the effect of query and document length on retrieval effectiveness with BM25 and BERT, and the reasons behind the substantial improvements introduced with BERT.
\myworries{Revisit after completing experimental results}
\item
We release an end-to-end pipeline, Birch\footnote{https://github.com/castorini/birch}, that applies BERT to document retrieval over large document collections via integration with the open-source Anserini information retrieval toolkit.
An accompanying Docker image is also included to ensure that anyone can easily deploy and test our system.
We elaborate on the technical challenges in the integration of NLP and IR capabilities, and the rationale behind design decisions.
\end{itemize}

\section{Thesis Organization}

\myworries{Update}
The remainder of this thesis is organized in the following order:\
Chapter \ref{ch:review} reviews related work in document retrieval, pretrained language models and document re-ranking models based on machine learning techniques.
Chapter \ref{ch:model} motivates the approach with some background information on the task, and introduces the datasets used for both training and evaluation as well as metrics.
Chapter \ref{ch:arch} proposes an end-to-end pipeline for document retrieval with BERT by elaborating on the design decisions and challenges.
Chapter \ref{ch:exp} describes the experimental setup, and presents the results on three newswire collections -- Robust04, Core17 and Core18.
Chapter \ref{ch:conclusion} concludes the thesis by summarizing the contributions and discussing future work.