%======================================================================
\chapter{Introduction}
%======================================================================

Document retrieval refers to the task of generating a ranking of typically unstructured text documents from a potentially large corpus $ D $ in response to a user query $ Q $.
The query representation is matched against a previously produced inverted index of the collection to determine candidate documents and compute a similarity score for each document.
The retrieved documents are returned in order of relevance based on some retrieval metric such as average precision (AP).

Document retrieval systems traditionally rely on term-matching techniques, such as BM25, to judge the relevance of documents in a corpus.
More specifically, these techniques favor documents that share the most common terms with the query.
As a result, document retrieval systems may fail to detect documents that do not contain the exact query terms, but are nonetheless directly relevant.
For example, ...
\myworries{Add figure?}

The classic approach to document retrieval clearly neglects to exploit rich semantic information embedded in the document texts.
This shortcoming has been the subject of NLP researchers for some time now:\
\myworries{Examples...}

With the advent of deep learning, numerous neural models that extract semantic matching signals to aid in document retrieval have also been proposed to by-pass the need to manually engineer natural language features.
These neural models are usually involved in multi-stage architectures where a list of candidate documents are retrieved with a standard bag-of-words term-matching technique as described above.
The documents in this list are then rescored and reranked by the neural model.
\myworries{Some notable examples include...}

Despite growing interest in neural networks, some researchers have recently voiced concern as to whether their use has truly contributed to progress in the field of information retrieval \myworries{citation}, at least in the absence of large amounts of behavioral log data only available to search companies.
...
\myworries{Take stuff from their neural hype paper}

One recent innovation that has changed the tide in NLP research has been massively pre-trained language models with its most popular example today being Bidirectional Encoder Representation Transformers (BERT) \cite{devlin2018bert}.
BERT has achieved state-of-the-art results across a wide range of NLP tasks from question answering to machine translation.
While BERT has enjoyed widespread adoption across the NLP community, its application in information retrieval research has been limited in comparison.
\myworries{Why?}


This thesis presents a novel way to successfully apply BERT to yield large improvements in ``ad hoc'' document retrieval on newswire articles.
Applying BERT to document retrieval on newswire collections requires solving two fundamental challenges.
First, relevance judgements are provided only at the document level in most collections.
That is, given a query, we only know what documents are relevant, not which specific spans within the documents are.
...
Second, most newswire documents exceed the length that BERT was designed to handle.

To address these challenges, we propose aggregating sentence-level evidence to rank documents instead of relying solely on document-level relevance judgements.
We overcome the lack of sentence level judgements by leveraging sentence- or passage-level judgements available in document collections in other domains to fine-tune BERT models.
Surprisingly, we demonstrate that models of relevance can be successfully transferred across domains.

\myworries{Reproducibility}
It is important to note that the representational power of neural networks come at the cost of challenges in interpretability.
For this reason, ...

\myworries{Integration challenges}
This system also highlights the need to bridge the worlds of natural language processing and information retrieval from a software engineering perspective in order to benefit from both.
On one hand, most deep learning toolkits today, including TensorFlow and PyTorch, are written in Python with a C++ backend.
On the other hand, the open-source search library that facilitates document retrieval in our system as well as many others, Lucene, is implemented in Java, and hence runs on the Java Virtual Machine (JVM).
Therefore, the integration between Python and the JVM presents a technical challenge to be addressed.

\myworries{TREC DL?}

\section{Contributions}

The main contributions of this thesis can be summarized as follows:\\

\begin{itemize}
\item
We present two innovations to successfully apply BERT to \textit{ad hoc} document retrieval with large improvements:\
integrating sentence-level evidence to address the fact that BERT cannot process long spans posed by newswire documents, and exploiting cross-domain models of relevance for collections without sentence- or passage-level annotations.
\item
We explore through various error analysis experiments on the effects of cross-domain relevance transfer with BERT as well as the contributions of BM25 and sentence scores to the final document ranking.
\item 
With the proposed model, we establish state-of-the-art effectiveness on three standard TREC newswire collections at the time of writing.\
\myworries{neural or otherwise}
\item
We release an end-to-end pipeline that applies BERT to document retrieval over large document collections via integration with the open-source Anserini information retrieval toolkit.
We elaborate on the technical challenges in the integration of NLP and IR capabilities, along with the design rationale behind our approach to tightly-coupled integration between Python to support neural networks and the Java Virtual Machine to support document retrieval using the open-source Lucene search library.
\myworries{something about demo, TREC DL...}

\end{itemize}

\section{Thesis Organization}

The remainder of this thesis is organized in the following order:\
\myworries{add link to actual chapters}
Chapter 2 reviews related work in neural document retrieval, particularly applications of BERT to document retrieval.
Chapter 3 motivates the approach with some background information on the task, and introduces the datasets used for both training and evaluation as well as metrics.
Chapter 4 proposes an end-to-end pipeline for document retrieval with BERT by elaborating on the design decisions and challenges.
\myworries{What about TREC DL? MS MARCO?}
Chapter 5 describes the experimental setup, and presents the results on three newswire collections -- Robust04, Core17 and Core18.
Chapter 6 concludes the thesis by summarizing the contributions and discussing future work.

%In the beginning, there was $\pi$:
%
%\begin{equation}
%   e^{\pi i} + 1 = 0  \label{eqn_pi}
%\end{equation}
%A \gls{computer} could compute $\pi$ all day long. In fact, subsets of digits of $\pi$'s decimal approximation would make a good source for psuedo-random vectors, \gls{rvec} . 
%
%%----------------------------------------------------------------------
%\section{State of the Art}
%%----------------------------------------------------------------------
%
%See equation \ref{eqn_pi} on page \pageref{eqn_pi}.\footnote{A famous equation.}
%
%\section{Some Meaningless Stuff}
%
%The credo of the \gls{aaaaz} was, for several years, several paragraphs of gibberish, until the \gls{dingledorf} responsible for the \gls{aaaaz} Web site realized his mistake:
%
%"Velit dolor illum facilisis zzril ipsum, augue odio, accumsan ea augue molestie lobortis zzril laoreet ex ad, adipiscing nulla. Veniam dolore, vel te in dolor te, feugait dolore ex vel erat duis nostrud diam commodo ad eu in consequat esse in ut wisi. Consectetuer dolore feugiat wisi eum dignissim tincidunt vel, nostrud, at vulputate eum euismod, diam minim eros consequat lorem aliquam et ad. Feugait illum sit suscipit ut, tation in dolore euismod et iusto nulla amet wisi odio quis nisl feugiat adipiscing luptatum minim nisl, quis, erat, dolore. Elit quis sit dolor veniam blandit ullamcorper ex, vero nonummy, duis exerci delenit ullamcorper at feugiat ullamcorper, ullamcorper elit vulputate iusto esse luptatum duis autem. Nulla nulla qui, te praesent et at nisl ut in consequat blandit vel augue ut.
%
%Illum suscipit delenit commodo augue exerci magna veniam hendrerit dignissim duis ut feugait amet dolor dolor suscipit iriure veniam. Vel quis enim vulputate nulla facilisis volutpat vel in, suscipit facilisis dolore ut veniam, duis facilisi wisi nulla aliquip vero praesent nibh molestie consectetuer nulla. Wisi nibh exerci hendrerit consequat, nostrud lobortis ut praesent dignissim tincidunt enim eum accumsan. Lorem, nonummy duis iriure autem feugait praesent, duis, accumsan tation enim facilisi qui te dolore magna velit, iusto esse eu, zzril. Feugiat enim zzril, te vel illum, lobortis ut tation, elit luptatum ipsum, aliquam dolor sed. Ex consectetuer aliquip in, tation delenit dignissim accumsan consequat, vero, et ad eu velit ut duis ea ea odio.
%
%Vero qui, te praesent et at nisl ut in consequat blandit vel augue ut dolor illum facilisis zzril ipsum. Exerci odio, accumsan ea augue molestie lobortis zzril laoreet ex ad, adipiscing nulla, et dolore, vel te in dolor te, feugait dolore ex vel erat duis. Ut diam commodo ad eu in consequat esse in ut wisi aliquip dolore feugiat wisi eum dignissim tincidunt vel, nostrud. Ut vulputate eum euismod, diam minim eros consequat lorem aliquam et ad luptatum illum sit suscipit ut, tation in dolore euismod et iusto nulla. Iusto wisi odio quis nisl feugiat adipiscing luptatum minim. Illum, quis, erat, dolore qui quis sit dolor veniam blandit ullamcorper ex, vero nonummy, duis exerci delenit ullamcorper at feugiat. Et, ullamcorper elit vulputate iusto esse luptatum duis autem esse nulla qui.
%
%Praesent dolore et, delenit, laoreet dolore sed eros hendrerit consequat lobortis. Dolor nulla suscipit delenit commodo augue exerci magna veniam hendrerit dignissim duis ut feugait amet. Ad dolor suscipit iriure veniam blandit quis enim vulputate nulla facilisis volutpat vel in. Erat facilisis dolore ut veniam, duis facilisi wisi nulla aliquip vero praesent nibh molestie consectetuer nulla, iriure nibh exerci hendrerit. Vel, nostrud lobortis ut praesent dignissim tincidunt enim eum accumsan ea, nonummy duis. Ad autem feugait praesent, duis, accumsan tation enim facilisi qui te dolore magna velit, iusto esse eu, zzril vel enim zzril, te. Nisl illum, lobortis ut tation, elit luptatum ipsum, aliquam dolor sed minim consectetuer aliquip.
%
%Tation exerci delenit ullamcorper at feugiat ullamcorper, ullamcorper elit vulputate iusto esse luptatum duis autem esse nulla qui. Volutpat praesent et at nisl ut in consequat blandit vel augue ut dolor illum facilisis zzril ipsum, augue odio, accumsan ea augue molestie lobortis zzril laoreet. Ex duis, te velit illum odio, nisl qui consequat aliquip qui blandit hendrerit. Ea dolor nonummy ullamcorper nulla lorem tation laoreet in ea, ullamcorper vel consequat zzril delenit quis dignissim, vulputate tincidunt ut."