%======================================================================
\chapter{Background and Related Work}
%======================================================================

\section{Unsupervised Language Models? BERT?}

\begin{figure}[b!]
\centering
  \includegraphics[width=5in]{classification.png}
\caption{BERT Sentence Pair Classification Model.}
\label{fig:bert}
\end{figure}

\myworries{Word embeddings}

These models
work by pre-training LSTM-based or transformer-based [19] lan-guage models on a large corpus, and then by performing minimal
task fine-tuning

With the increasing availability of large corpora, pretrained deep language models have been rapidly gaining traction among NLP researchers.
Recent work in NLP has demonstrated that language model pretraining has proven extremely effective for many natural language processing tasks ranging from machine translation to reading comprehension.
One of the latest and most sophisticated pretrained deep language models is undoubtedly the Bidirectional Encoder Representations from Transformers (BERT), which has already enjoyed widespread popularity across the NLP community.~\cite{devlin2018bert}
Unlike previous language models, such as OpenAI's Generative Pretrained Transformer (GPT)~\cite{radford2019language}, BERT produces deep bidirectional representations by conditioning on both left and right context in all layers by employing a new pretraining objective called ``masked language model'' (MLM).
Conceptually, MLM randomly masks some of the tokens from the input with the goal of predicting the original token based only on its left and right context.

As expected, optimizing this objective requires a very complex model:\ for example, the larger BERT model requires around 340 million parameters be optimized.
In fact, training this model end-to-end takes four days to complete even on 16 high-end tensor processing units (TPUs)~\cite{devlin2018bert}.
Fortunately, there exists a technique to benefit from these models without having to train an entire model from scratch.
The most versatile and widely adopted approach to applying these neural models to downstream NLP tasks is based on ``freezing'' their last layer, and ``fine-tuning'' on external data for the specific task.
Not only does this approach introduce only a few task-specific parameters to optimize, but it also greatly boosts the performance of many NLP tasks given the rich semantic expressiveness introduced by the pretrained language model.
Figure \ref{fig:bert} visualizes the input and output for fine-tuning BERT for a ``sentence pair classification'' model.
To form an input, two sequences of tokens are concatenated with the meta-token $[SEP]$, i.e: separator, and prepended with $[CLS]$, corresponding to the ``class'' meta-token.
A single-layer neural network is added to the end of this network with the class label as the input, and subsequently trained for the specific downstream task.

\myworries{Success in other tasks}

\section{Unsupervised Document Retrieval}

\subsection{Okapi BM25}

\myworries{Binary and its extension to BM25... shortcomings semantic}

Okapi BM25 (commonly dubbed BM25) is a bag-of-words ranking function that was developed to accommodate documents of variable lengths.
BM25 ranks documents based on the occurrence of query terms in each document, paying more attention to the rarer terms in the query.
The goal of this approach is to take into account term frequency and document frequency while estimating the relevance of a document for a given query without introducing too many additional parameters. (Sparck)
To achieve this goal, BM25 implementations define two parameters for term frequency saturation and field-length normalization, respectively.
\myworries{Tuned for most common datasets?}

\subsection{RM3}

RM3 is a pseudo-relevance feedback mechanism where the original query is expanded by adding terms found in the contents of relevant BM25 documents.
\myworries{Why is it effective, why does it improve performance, how common is it}

\myworries{Previous approaches and results on these datasets}
\myworries{Cormack et al and other stuff}

\section{Neural Document Retrieval}

\begin{figure}[t!]
\centering
  \includegraphics[width=6in]{deep_matching.png}
\caption{\cite{guo2017drmm}}
\label{fig:deep_matching}
\end{figure}

\myworries{concerns}
Neural models developed to address the deep matching problem can be divided into two broad categories based on their underlying architecture: representation-based and interaction-based.

\subsection{Representation-based Models}

Representation-based approaches first construct the representation from the input vectors for each text, e.g: documents, with a deep neural network, and then perform matching between the representations.
Earlier work on neural information retrieval focused on representation-based approaches, such as DSSM \cite{huang2013learning} and C-DSSM \cite{shen2014learning}.
DSSM (short for Deep Structured Semantic Models) \cite{huang2013learning} extended previously dominant latent semantic models to deep semantic matching for web search by projecting query and documents into a common low-dimensional space.
In order to accommodate a large vocabulary required by the task, the text sequences were mapped into character-level trigrams with a word hashing layer before computing a similarity matrix through dot product and softmax layers.
While shown effective on a private? datasset comprised of log files of a commercial search engine, DSSM was criticized by Guo et al. \cite{guo2017drmm} for requiring too much training data.
Moreover, DSSM cannot match synonyms because it is based on the specific composition of words.
C-DSSM \cite{shen2014learning} extended DSSM by introducing a convolutional layer to devise semantic vectors for search queries and Web document.
By performing a max pooling operation to extract local contextual information at the n-gram level, a global vector representation is formed from the local features.
They demonstrated that both local and global contextual features were necessary for semantic matching for Web search.
While C-DSSM improves over DSSM by exploiting the context of each trigram, it still suffers from the same issues listed above.
%However, ARC-I was mainly designed for short texts and ...

\subsection{Interaction-based Models}

Interaction-based approaches instead capture local matching signals, and directly compute the similarity of the query and document representations.
In contrast to more shallow representation-based approaches, in this approach deep neural network learns more complex hierarchical matching patterns.
Some notable examples include DRMM \cite{guo2017drmm}, KNRM \cite{xiong2017knrm} and DUET \cite{mitra2017neural}.
DRMM (stands for Deep Relevance Matching Model) \cite{guo2017drmm} maps the variable-length local interactions of each query term with the document? into a fixed-length matching histogram.
A feed forward matching network is used to learn hierarchical matching patterns and compute a matching score is computed for each term.
An overall matching score is obtained by aggregating the scores from each query term with a term gating network.
Similar to other models, KNRM \cite{xiong2017knrm} calculates the word-word similarities between query and document embeddings.
They propose a novel kernel-pooling technique to convert word-level interactions into ranking features.
\myworries{Details?}
Finally they combine the ranking features into a final ranking score through a learning-to-rank layer.
\myworries{Private dataset and stuff? Benefits?}
Unlike DRMM and KNRM, the goal of DUET \cite{mitra2017neural} is to employ both local and distributed representations to leverage both exact matching and semantic matching signals.
DUET is composed of two separate deep neural networks, one to match the query and the document using a one-hot representation, and another using learned distributed representations, which are trained jointly.
The former estimates document relevance based on exact matches of the query terms in the document by computing an interaction matrix from one-hot encodings.
The latter instead performs semantic matching by computing the element-wise product between the query and document embeddings.
Their approach was shown to significantly outperform traditional baselines for web search with lots of clickthrough logs.
%Mitra et al. \cite{mitra2017neural} show that distributed representations indeed improve over 
\myworries{Any other major models in the tutorial?}

%other models available in MatchZoo, such as ARC-I [9], MV-LSTM [19], and aNMM [21] were mainly designed for short texts and not geared towards handling documents (which are much longer)
%Note that some of the models discussed in this section address the web search task where a variety of other signals are available, such as large amounts of log data and the webgraph.
%This stands in contrast to the ``ad hoc'' document retrieval task where available data is limited only to the document text, which is the main focus of this thesis.
In fact, despite growing interest in neural models for document ranking, researchers have recently voiced concern as to whether or not they have truly contributed to progress~\cite{lin2019neural}, at least in the absence of large amounts of behavioral log data only available to search engine companies.
This opinion piece echoes the general skepticism concerning the empirical rigor and contributions of machine learning applications in Lipton et al. \cite{lipton2018troubling} and Sculley at al. \cite{sculley2018winner}.
%In particular, Lin et al.~\cite{lin2019neural} lament that comparisons to weak baselines sometimes inflate the merits of certain neural information retrieval methods.

To rigorously test this claim, Yang et al. \cite{Yang_etal_SIGIR2019} recently conducted a thorough meta-analysis of over 100 papers that report results on the TREC Robust 2004 Track.
Their findings are illustrated in Figure \ref{fig:neural_robust04} where the solid black line represents the best submitted run at 0.333, and the dotted black line the median TREC run at 0.258.
The other line is a RM3 baseline run with default parameters from the Anserini open-source information retrieval toolkit~\cite{Yang:2018:ARR:3289400.3239571} at AP 0.3903.
The untuned RM3 baseline is more effective than ~60\% of all studied papers, and ~20\% of them report results below the TREC median.
More surprisingly, only six of them report AP scores higher than the TREC best, with the highest being by Cormack et al.~\cite{Cormack:2009:RRF:1571941.1572114} in 2009 at AP 0.3686.
Among the neural models, the highest encountered score is by Zamani et al.~\cite{zamani2018neural} in 2018 at AP 0.2971.
%Despite the upward trend in the effectiveness of neural approaches, 

Yang et al. also implemented five recent neural retrieval models discussed above to evaluate their effectiveness on the ``ad hoc'' document retrieval task on the Robust04 dataset: DSSM, CDSSM, DRMM, KNRM and DUET.
These models were selected because they were specifically designed for ``ad hoc'' document retrieval unlike some others designed to handle shorter texts?
\myworries{All the models were trained on the documents in the baseline RM3 runs... details CV etc}
Table \ref{tab:exp-robust04} displays the AP and NDCG@20 values of each run on the Robust04 dataset.
The first two rows are taken from the original DRMM paper~\cite{guo2017drmm} and show their reported baseline and results; the other models do not report results on Robust04.
The next row refers to the untuned RM3 baseline from Anserini.
The following results refer to results from the neural models that were used to rerank the strong baseline, BM25+RM3, to gauge how much they actually contribute.
Of the five models, only one -- DRMM -- is found to significantly improve over the baseline.
%Mitra and Craswell [14] classified DRMM as a lexi- cal matching modeling (in fact, the model explicitly captures tfand idf). DUET is a hybrid lexical/semantic matching model, while the others are semantic matching primarily. One possible interpretation of our findings is that TREC judgments alone are not sufficient to train semantic matching models

\begin{table}[t]
\vspace{0.2cm}
\centering
\begin{tabular}{lll}
\toprule
\textbf{Condition} \mbox{\hspace{0.5cm}} & \textbf{AP} \mbox{\hspace{1.0cm}} & \textbf{NDCG@20} \\ 
\toprule
BM25~\cite{guo2016deep} & 0.255 & 0.418 \\
DRMM~\cite{guo2016deep} & 0.279 & 0.431 \\
%Paper 2    & 0.272       & - \\
BM25+RM3   & 0.3033      & 0.4514     \\
~+ DSSM    & 0.3026      & 0.4491     \\  
~+ CDSSM   & 0.2995      & 0.4468     \\
~+ DRMM    & 0.3152$^{\dagger}$  & 0.4718$^{\dagger}$ \\  
~+ KNRM    & 0.3036      & 0.4441     \\  
~+ DUET    & 0.3051      & 0.4502     \\ 
\bottomrule
\end{tabular}
\vspace{0.2cm}
\caption{Experimental results applying neural models to rerank a strong baseline; $^{\dagger}$ indicates statistical significance.}% at the $p<0.01$ level.}
\label{tab:exp-robust04}
\vspace{-0.6cm}
\end{table}

\begin{figure}[b!]
\centering
  \includegraphics[width=6in]{neural_robust04.png}
\caption{...}
\label{fig:neural_robust04}
\end{figure}

\subsection{Contextualized Language Models}

While the neural ranking models introduced in the previous section successfully leverage contextual information to improve retrieval effectiveness, they are limited by the size and variability of the available training data.
Ideally, these models would be trained on a large number of semantically varied yet relevant query-document pairs; however, it is impractical to automatically gather a sufficient number of such training samples.
Massively pretrained unsupervised language models hold promises for obtaining better representations for the query and document, and therefore, achieving unprecedented effectiveness at semantic matching without the need for more relevance information.
Language models are pretrained on massive amounts of external data in an unsupervised fashion.
\myworries{ELMo, GPT, BERT...}

Document retrieval requires an understanding of the relationship between two text sequences -- the query and the document.
However, language modeling does not suffice to capture such a relationship.
BERT facilitates such relevance classification by pre-training a binary next sentence prediction task based on its masking language model approach.
This mechanism has been adopted for ``passage'' reranking by ...
Notably, Nogueira et al. \cite{nogueira2019passage} proposed to re-rank MS MARCO passages based on a simple re-implementation of BERT, outperforming the previous state of the art by 27\% in MRR@10 and replacing the top entry in the leaderboard of the MS MARCO passage retrieval task at the time of publication.
Our neural model is inspired by the BERT re-implementation described in this paper.
\myworries{What about others? MS MARCO leaderboard?}

To our knowledge, Yang et al. \cite{yang2019simple} were the first to successfully apply BERT to ``ad hoc'' document retrieval.
They demonstrated that BERT can be fine-tuned to capture relevance matching by applying the above approach on the TREC Microblog Tracks where document length does not pose an issue.
They further proposed overcoming the challenge of long documents by applying inference on each individual sentence and combining the top scores to compute a final document score.
Their approach was motivated by user studies? by Zhang et al. \cite{zhang2018effective} who suggested that the most relevant sentence or paragraph in a document provides a good proxy for overall document relevance.
Their work paved the way for future work that culminated in this thesis.

More recently, MacAvaney et al. \cite{macavaney2019cedr} shifted focus from incorporating BERT as a reranker to using its representation capabilities to improve existing neural architectures.
By computing a relevance matrix between the query and each candidate document? at each layer of a contextualized language model -- ELMo or BERT --  they established state-of-the-art effectiveness on Robust04 and Webtrack 2012--2014 at the time of writing.
\myworries{Not MAP...}
They also proposed a joint model that combines the aforementioned classification mechanism of BERT into existing neural architectures.
They claim that this approach benefits from both deep semantic matching with BERT \textit{and} relevance batching with traditional ranking architectures.

\myworries{Check out and mention Qiao:1904.07531:2019,Padigela:1905.01758:2019 and others}

\section{Evaluation Metrics}

The standard approach to evaluation in information retrieval relies on the distinction between ``relevant'' and ``irrelevant'' documents with respect to an information need as expressed by a query.
A number of automatic evaluation metrics has been formalized for ranking tasks such as document retrieval.
These metrics rely on 
The size of most document collections makes it infeasible for humans to manually judge the relevance of all documents.
All relevant documents need to be labelled to prevent false negatives, i.e: treating documents which are in fact relevant as irrelevant.

\subsection{Mean Average Precision (MAP)}

Precision specifies what fraction of a set of retrieved documents is in fact relevant for a given query $ q $.
Precision can easily be extended to evaluate ranked retrieval results by ...
Average precision (AP) expresses the average of the precision values obtained for the set of top $ k $ documents for a query.
Support that $ D = \{d_1, ..., d_{m_j}\} $ is the set of all relevant documents for a query $ q_j $, then AP can be formulated as:

\begin{equation}
AP = \frac{1}{m_j} \sum^{m_j} _{k = 1} P(R_{jk})
\end{equation}

where $ R_{jk} $ represents the set of top $ k $ tanked retrieval results.

The respective AP for each query can be aggregated to obtain mean average precision (MAP) for the overall retrieval effectiveness in the form of a single-figure measure of quality across various recall levels:

\begin{equation}
MAP = \frac{\sum^{|Q|} _{j = 1} AP}{Q} = \frac{1}{Q} \sum^{|Q|} _{j = 1} \frac{1}{m_j} \sum^{m_j} _{k = 1} P(R_{jk})
\end{equation}

%\myworries{Meaning of MAP: To recapitulate, if the AP is 0.5, the relevant
%answer occurs at every second position. If the AP score is 0.3, the relevant answer occurs
%at every 3rd position and an AP score of 0.1 would mean that every 10th answer is correct.}

It has been show to have especially good discrimination and stability compared to other metrics, which makes it the ideal choice for large text collections \cite{manning2010introduction}.
It is hence one of the standard metrics among the TREC community.

\subsection{Precision at k (P@k)}

Unlike MAP which factors in precision at all recall levels, certain applications have a distinctly different notion for ranking quality.
Particularly in the case of web search, the user often only cares about the results on the first page or two, but not all of them.
This restriction essentially leads to measuring precision at fixed low levels of retrieved results, i.e: top $ k $ documents -- hence the name for metric ``precision at $ k $''.
On the one hand, it eliminates the need for any estimate of the size of the set of relevant documents.
However, it also produces the least stable out of all measures.
Moreover, precision at $ k $ does not average well because the total number of relevant documents for a query has a very strong influence on its value. 

\subsection{Normalized Discounted Cumulative Gain (NDCG@20)}

Cumulative gain (CG) simply computes the sum of relevance labels for all the retrieved documents.
It views the search results as an unordered set, and disregards the ordering of the documents.
Since a highly relevant document is inherently more useful when it appears higher up in the search results, CG has been extended to discounted cumulative gain (DCG).
Discounted cumulative gain (DCG) estimates the relevance of a document based on its rank among the retrieved documents.
The relevance measure is accumulated from top to bottom, discounting the value of documents at lower ranks.
NDCG measures DCG for the top $ k $ documents, normalizing by the highest possible value for a query.

NDCG is uniquely useful in applications with a non-binary notion of relevance, e.g: a spectrum of relevance. 
For this reason, NDCG is a popular choice for systems with machine learning approaches?
Like precision at k, it is evaluated as a weighted sum over the top k search results, and normalized so that a perfect ranking yields NDCG equals 1.
This makes NDCG comparable across different queries:\
The NDCG values for all queries can be averaged to reliably evaluate the effectiveness of a ranking algorithm for various information needs across a collection.
However, the use of NDCG is dependent on the availability of ground truth relevance labels?

\section{Datasets}

\myworries{Add statistics and examples}

\myworries{Elaborate on splits}

\subsection{Fine-Tuning}

As discussed in Section \ref{intro}, applying BERT to document retrieval requires leveraging passage- or sentence-level relevance judgements fortuitously available in large text collections.
Since no such newswire collection currently exists, we train the BERT relevance classifier on three out-of-domain collections.

\subsubsection{TREC Microblog}

TREC Microblog datasets draw from the Microblog Tracks at TREC from 2011 to 2014, with topics (i.e., queries) and relevance judgments over tweets. We use the dataset prepared by Rao et al. (2019)

\subsubsection{MS MARCO}

MS MARCO features user queries sampled from Bingâ€™s search logs and passages extracted from web documents. Each query is associated with sparse relevance judgments by human editors.

\subsubsection{TREC CAR}

TREC CAR uses queries and paragraphs extracted from English Wikipedia: each query is formed by concatenating an article title and a section heading, and passages in that section are considered relevant.
This makes CAR, essentially, a synthetic dataset.

\subsection{Evaluation}

We conduct end-to-end document ranking experiments on three TREC newswire collections:\
the Robust Track from 2004 (Robust04) and the Common Core Tracks from 2017 and 2018 (Core17 and Core18).

\subsubsection{Robust04}

Robust04 comprises 250 topics, with relevance judgments on a collection of 500K documents (TREC Disks 4 and 5).

\subsubsection{Core17 \& Core18}
 
Core17 and Core18 have only 50 topics each; the former uses 1.8M articles from the New York Times Annotated Corpus while the latter uses around 600K articles from the TREC Washington Post Corpus.